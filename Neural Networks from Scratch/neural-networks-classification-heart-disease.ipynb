{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13397267,"sourceType":"datasetVersion","datasetId":8501745}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Cleveland Heart Disease Classification with Neural Networks","metadata":{"id":"vz3AFG0s5xK7"}},{"cell_type":"markdown","source":"## 1. Import libraries, datasets and observe the data","metadata":{"id":"lZr0NoxC53Qk"}},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T19:12:56.665160Z","iopub.execute_input":"2025-10-15T19:12:56.665473Z","iopub.status.idle":"2025-10-15T19:12:56.670464Z","shell.execute_reply.started":"2025-10-15T19:12:56.665451Z","shell.execute_reply":"2025-10-15T19:12:56.669386Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport seaborn as sns\nimport numpy as np\nimport matplotlib.pyplot as plt","metadata":{"id":"rXgatbZ05Oa4","trusted":true,"execution":{"iopub.status.busy":"2025-10-15T19:11:37.751184Z","iopub.execute_input":"2025-10-15T19:11:37.751750Z","iopub.status.idle":"2025-10-15T19:11:39.306232Z","shell.execute_reply.started":"2025-10-15T19:11:37.751709Z","shell.execute_reply":"2025-10-15T19:11:39.305204Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# in Google Colab:\n# df = pd.read_csv('/content/drive/MyDrive/Datasets/heart.csv')\n\n# in Kaggle\ndf = pd.read_csv('/kaggle/input/cleveland-heat-disease/heart.csv')","metadata":{"id":"au1OemMi5vR4","trusted":true,"execution":{"iopub.status.busy":"2025-10-15T19:11:43.394279Z","iopub.execute_input":"2025-10-15T19:11:43.394776Z","iopub.status.idle":"2025-10-15T19:11:43.426401Z","shell.execute_reply.started":"2025-10-15T19:11:43.394747Z","shell.execute_reply":"2025-10-15T19:11:43.425208Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.head()","metadata":{"id":"neVpc2VR5vXq","outputId":"a44a6ce2-4508-4f9b-a852-0a60630e7a54","trusted":true,"execution":{"iopub.status.busy":"2025-10-15T19:11:45.824905Z","iopub.execute_input":"2025-10-15T19:11:45.825803Z","iopub.status.idle":"2025-10-15T19:11:45.857636Z","shell.execute_reply.started":"2025-10-15T19:11:45.825771Z","shell.execute_reply":"2025-10-15T19:11:45.856630Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Shape of the data\ndf.shape","metadata":{"id":"KGqFcs-P5vZU","outputId":"6b8a94c1-76f6-428d-b265-a33a23e709e2","trusted":true,"execution":{"iopub.status.busy":"2025-10-15T19:11:48.026105Z","iopub.execute_input":"2025-10-15T19:11:48.027243Z","iopub.status.idle":"2025-10-15T19:11:48.033419Z","shell.execute_reply.started":"2025-10-15T19:11:48.027206Z","shell.execute_reply":"2025-10-15T19:11:48.031874Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The data has 1025 samples (rows) and 14 features (columns). The features are following:\n\n1. `age` - person's age in years\n2. `sex` - the person's sex (1 = male, 0 = female)\n3. `cp` chest main type - the chest pain experienced. Value 1: typical angina; Value 2: atypical angina; Value 3: non-anginal pain; Value 4: asymptomatic\n4. `trestbps` Resting blood pressure - the person's resting blood pressure\n5. `chol` cholesterol - the person's cholesterol measurement in mg/dl\n6. `fbs` Fasting blood sugar - the person's fasting blood sugar (>120 mg/dl, 1 = true, 0 = false)\n7. `restecg` resting ecg - resting electrocardiographic measurement.\n8. `thalach`- max heart rate achieved\n9. `exang`- exercise induced angina (1 = yes, 0 = no)\n10. `oldpeak`-\n11. `slope`\n12. `ca` - number of vessels\n13. `thal` - thalasemia\n14. `target` - diagnosis of heart disease","metadata":{"id":"Vd_G4gbv6lHt"}},{"cell_type":"code","source":"df.describe().T","metadata":{"id":"CMHL5rPw5vbh","outputId":"e49f5b3f-112e-4667-f12c-44b2bc3e9ef1","trusted":true,"execution":{"iopub.status.busy":"2025-10-15T19:11:50.485735Z","iopub.execute_input":"2025-10-15T19:11:50.486195Z","iopub.status.idle":"2025-10-15T19:11:50.541476Z","shell.execute_reply.started":"2025-10-15T19:11:50.486165Z","shell.execute_reply":"2025-10-15T19:11:50.540278Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.info()","metadata":{"id":"tjBB98LT5vdC","outputId":"bfee43da-d14a-4f34-dcdc-726a4037b858","trusted":true,"execution":{"iopub.status.busy":"2025-10-15T19:11:53.091407Z","iopub.execute_input":"2025-10-15T19:11:53.091761Z","iopub.status.idle":"2025-10-15T19:11:53.129393Z","shell.execute_reply.started":"2025-10-15T19:11:53.091734Z","shell.execute_reply":"2025-10-15T19:11:53.128171Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"All feastures are numeric columns.","metadata":{"id":"tKcQFahj7t3e"}},{"cell_type":"markdown","source":"## 2. Check and handle missing values","metadata":{"id":"2FCmm5n77yrm"}},{"cell_type":"code","source":"pd.DataFrame(df.isnull().sum(), columns = ['Missing']).T","metadata":{"id":"6vw8HCDg5vfJ","outputId":"13da1d0e-d1ca-45a5-ef0a-af364799bd9b","trusted":true,"execution":{"iopub.status.busy":"2025-10-15T19:11:56.561136Z","iopub.execute_input":"2025-10-15T19:11:56.561437Z","iopub.status.idle":"2025-10-15T19:11:56.574145Z","shell.execute_reply.started":"2025-10-15T19:11:56.561414Z","shell.execute_reply":"2025-10-15T19:11:56.573076Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"There is not any missing values in all columns.","metadata":{"id":"A1GrEUbe8QJK"}},{"cell_type":"code","source":"","metadata":{"id":"hdClrrP45vgx","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3. Distribution and outliers","metadata":{"id":"0tJh4TEG8ULa"}},{"cell_type":"code","source":"import math","metadata":{"id":"B3HDYhusu2dP","trusted":true,"execution":{"iopub.status.busy":"2025-10-15T19:11:59.600189Z","iopub.execute_input":"2025-10-15T19:11:59.600581Z","iopub.status.idle":"2025-10-15T19:11:59.604928Z","shell.execute_reply.started":"2025-10-15T19:11:59.600554Z","shell.execute_reply":"2025-10-15T19:11:59.604118Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cols = df.columns.to_list()\nncols = 3\nnrows = int(np.ceil(len(cols) / ncols))\n\nfig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(15, 10))\naxes = axes.flatten()\n\nsns.set_theme(style=\"whitegrid\", palette=\"pastel\", font_scale=1.1)\n\nfor i, col in enumerate(cols):\n    sns.boxplot(data=df, x=col, ax=axes[i], color=sns.color_palette(\"pastel\")[1])\n    axes[i].set_xlabel(col, fontsize=10, weight='bold')\n    axes[i].set_ylabel(\"\")\n    axes[i].tick_params(axis='x', labelrotation=30)  # Rotate labels to avoid overlap\n    axes[i].set_title(\"\")  # Keep the plot clean, rely on x-labels\n\n# Remove unused axes\nfor j in range(i + 1, len(axes)):\n    fig.delaxes(axes[j])\n\nfig.suptitle(\"Boxplots for Each Feature\", fontsize=16, weight='bold', y=1.02)\nplt.tight_layout()\nplt.show()","metadata":{"id":"tL7BE1t65viz","outputId":"a3dbbc20-955f-470b-ad90-4df7b069e9ac","trusted":true,"execution":{"iopub.status.busy":"2025-10-15T19:11:59.976803Z","iopub.execute_input":"2025-10-15T19:11:59.977515Z","iopub.status.idle":"2025-10-15T19:12:01.648361Z","shell.execute_reply.started":"2025-10-15T19:11:59.977487Z","shell.execute_reply":"2025-10-15T19:12:01.647231Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"numeric_cols = ['trestbps', 'chol']","metadata":{"id":"TE1co8D85vkQ","trusted":true,"execution":{"iopub.status.busy":"2025-10-15T19:12:04.583934Z","iopub.execute_input":"2025-10-15T19:12:04.585297Z","iopub.status.idle":"2025-10-15T19:12:04.591670Z","shell.execute_reply.started":"2025-10-15T19:12:04.585242Z","shell.execute_reply":"2025-10-15T19:12:04.590056Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def outlier_detection(df, col):\n    q1 = np.percentile(df, 25)\n    q3 = np.percentile(df, 75)\n    iqr = q3 - q1\n\n    lower, upper = q1-1.5*iqr, q3 + 1.5*iqr\n\n    return lower, upper","metadata":{"id":"3Wx82BG5skR6","trusted":true,"execution":{"iopub.status.busy":"2025-10-15T19:12:04.874554Z","iopub.execute_input":"2025-10-15T19:12:04.874925Z","iopub.status.idle":"2025-10-15T19:12:04.881312Z","shell.execute_reply.started":"2025-10-15T19:12:04.874897Z","shell.execute_reply":"2025-10-15T19:12:04.879853Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for col in numeric_cols:\n    lower, upper = outlier_detection(df, col)\n\n    df[col] = np.clip(df[col], a_min = lower, a_max = upper)","metadata":{"id":"r3KXFmRlskNe","trusted":true,"execution":{"iopub.status.busy":"2025-10-15T19:12:09.930780Z","iopub.execute_input":"2025-10-15T19:12:09.931121Z","iopub.status.idle":"2025-10-15T19:12:09.944345Z","shell.execute_reply.started":"2025-10-15T19:12:09.931097Z","shell.execute_reply":"2025-10-15T19:12:09.943399Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"WA9XPq4SskFN"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"6i_taoLbskDU"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Distribution","metadata":{"id":"inU8a9wiu-mK"}},{"cell_type":"code","source":"cols = df.columns.to_list()\nncols = 3\nnrows = int(np.ceil(len(cols) / ncols))\n\nfig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(16, nrows * 3))\naxes = axes.flatten()\n\nsns.set_theme(style=\"whitegrid\", palette=\"pastel\", font_scale=1.1)\n\nfor i, col in enumerate(cols):\n    sns.histplot(data=df, x=col, ax=axes[i], color=sns.color_palette(\"pastel\")[1])\n    axes[i].set_xlabel(col, fontsize=10, weight='bold')\n    axes[i].set_ylabel(\"\")\n    axes[i].tick_params(axis='x', labelrotation=30)  # Rotate labels to avoid overlap\n    axes[i].set_title(\"\")  # Keep the plot clean, rely on x-labels\n\n# Remove unused axes\nfor j in range(i + 1, len(axes)):\n    fig.delaxes(axes[j])\n\nfig.suptitle(\"Boxplots for Each Feature\", fontsize=16, weight='bold', y=1.02)\nplt.tight_layout()\nplt.show()","metadata":{"id":"AErlUeV-vz5e","outputId":"2b3371d3-47cd-48b3-d29f-38ec52757236","trusted":true,"execution":{"iopub.status.busy":"2025-10-15T19:13:03.445463Z","iopub.execute_input":"2025-10-15T19:13:03.445769Z","iopub.status.idle":"2025-10-15T19:13:07.115762Z","shell.execute_reply.started":"2025-10-15T19:13:03.445745Z","shell.execute_reply":"2025-10-15T19:13:07.114543Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"wiKzmH3Ivz1v"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.head()","metadata":{"id":"yMyRHp8Lvzz1","outputId":"c2c5a5e9-17e9-45a0-b3a9-f9261c75a086","trusted":true,"execution":{"iopub.status.busy":"2025-10-15T19:13:14.082735Z","iopub.execute_input":"2025-10-15T19:13:14.083421Z","iopub.status.idle":"2025-10-15T19:13:14.104446Z","shell.execute_reply.started":"2025-10-15T19:13:14.083385Z","shell.execute_reply":"2025-10-15T19:13:14.102987Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 4. Scaling of numeric variables","metadata":{"id":"0Ua2DDp_uyrD"}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler","metadata":{"id":"ZRPyQRfIeAPF","trusted":true,"execution":{"iopub.status.busy":"2025-10-15T19:13:18.352076Z","iopub.execute_input":"2025-10-15T19:13:18.352399Z","iopub.status.idle":"2025-10-15T19:13:18.578286Z","shell.execute_reply.started":"2025-10-15T19:13:18.352377Z","shell.execute_reply":"2025-10-15T19:13:18.577332Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X = df.drop('target', axis = 1).values\ny = df['target'].values.reshape(-1,1)","metadata":{"id":"RTROfC8BdlP8","trusted":true,"execution":{"iopub.status.busy":"2025-10-15T19:13:19.094891Z","iopub.execute_input":"2025-10-15T19:13:19.095973Z","iopub.status.idle":"2025-10-15T19:13:19.104779Z","shell.execute_reply.started":"2025-10-15T19:13:19.095944Z","shell.execute_reply":"2025-10-15T19:13:19.103310Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","metadata":{"id":"BvjMWfDKd3NK","trusted":true,"execution":{"iopub.status.busy":"2025-10-15T19:13:21.020698Z","iopub.execute_input":"2025-10-15T19:13:21.021158Z","iopub.status.idle":"2025-10-15T19:13:21.027958Z","shell.execute_reply.started":"2025-10-15T19:13:21.021135Z","shell.execute_reply":"2025-10-15T19:13:21.026657Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"scaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","metadata":{"id":"F769Wc0Tvzxc","trusted":true,"execution":{"iopub.status.busy":"2025-10-15T19:13:21.898830Z","iopub.execute_input":"2025-10-15T19:13:21.899218Z","iopub.status.idle":"2025-10-15T19:13:21.925886Z","shell.execute_reply.started":"2025-10-15T19:13:21.899194Z","shell.execute_reply":"2025-10-15T19:13:21.924490Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"t73VEnKye5PR"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"BJnkSRZee5NT"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 6. Build model","metadata":{"id":"4q4FSnkkws4a"}},{"cell_type":"code","source":"# Initialization\n\nnp.random.seed(42)\nn_x = X_train.shape[1]   # should be 13\nn_h1 = 6\nn_h2 = 4\nn_y = 1","metadata":{"id":"pE_cDTrq7lxk","trusted":true,"execution":{"iopub.status.busy":"2025-10-15T19:13:25.268777Z","iopub.execute_input":"2025-10-15T19:13:25.269135Z","iopub.status.idle":"2025-10-15T19:13:25.274561Z","shell.execute_reply.started":"2025-10-15T19:13:25.269111Z","shell.execute_reply":"2025-10-15T19:13:25.273502Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Explanation of the code:\n\n1. `X_train.shape[1]` - extract the number of the features and set it to input size. Because, each neuron in the input layer corresponds to one feature.\n2. `n_h1 = 6` and `n_h2 = 4` - defines the number of neurons in the 1st and second hidden layers, respectively. these are design choices known as hyperparameters. They’re not fixed by theory but rather chosen based on experimentation, balancing model capacity and overfitting risk.\n3. `n_y` - defines the output layer with a single neuron; because it is binary classification task (result can be either 0 or 1).\n","metadata":{"id":"A1bXZoRY8se5"}},{"cell_type":"code","source":"X_train.shape","metadata":{"id":"mCdYf-3O7lkd","outputId":"92208271-8b1a-4a17-a6cf-127872589b14","trusted":true,"execution":{"iopub.status.busy":"2025-10-15T19:13:28.633375Z","iopub.execute_input":"2025-10-15T19:13:28.633695Z","iopub.status.idle":"2025-10-15T19:13:28.640205Z","shell.execute_reply.started":"2025-10-15T19:13:28.633673Z","shell.execute_reply":"2025-10-15T19:13:28.639187Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Weight initialization (small random numbers)\nW1 = np.random.randn(n_x, n_h1) * 0.1\nb1 = np.zeros((1, n_h1))\n\nW2 = np.random.randn(n_h1, n_h2) * 0.1\nb2 = np.zeros((1, n_h2))\n\nW3 = np.random.randn(n_h2, n_y) * 0.1\nb3 = np.zeros((1, n_y))","metadata":{"id":"Qu-QVqa_9thC","trusted":true,"execution":{"iopub.status.busy":"2025-10-15T19:13:29.283244Z","iopub.execute_input":"2025-10-15T19:13:29.283565Z","iopub.status.idle":"2025-10-15T19:13:29.289947Z","shell.execute_reply.started":"2025-10-15T19:13:29.283542Z","shell.execute_reply":"2025-10-15T19:13:29.288592Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The codes above initialize the weights and biases - which are the adjustable parameters that the network will learn during training. For each layer i, the matrix W (weights) connects layer i−1 to layer i, and b (biases) shifts the activation before it’s passed to the next layer.\n-  `W1 = np.random.randn(n_x, n_h1) * 0.1` creates a 13×6 weight matrix drawn from a standard normal distribution (randn gives mean 0, variance 1) and scaled by 0.1 to make the initial values small—this prevents early layer activations from exploding.\n- The bias `b1 = np.zeros((1, n_h1))` initializes as zeros, giving every neuron in that layer the same initial offset (biases can safely start at zero because random weights already break symmetry)\n\nThe same logic applies to W2 and b2 (6×4 matrix) for the second hidden layer, and W3 and b3 (4×1 matrix) for the output layer.\n\nThe random initialization gives each neuron a unique starting point so they learn distinct features rather than identical patterns. The scaling factor (0.1) ensures the network starts in a numerically stable regime, allowing gradients to flow effectively during backpropagation.\n","metadata":{"id":"iZ6O2LPA9qwv"}},{"cell_type":"code","source":"# Sigmoid activation function\n\ndef sigmoid(z):\n    return 1.0 / (1.0 + np.exp(-z))","metadata":{"id":"xuN3Elke7li_","trusted":true,"execution":{"iopub.status.busy":"2025-10-15T19:13:32.477347Z","iopub.execute_input":"2025-10-15T19:13:32.477673Z","iopub.status.idle":"2025-10-15T19:13:32.482979Z","shell.execute_reply.started":"2025-10-15T19:13:32.477649Z","shell.execute_reply":"2025-10-15T19:13:32.482090Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The sigmoid function is one of the most commonly used activation functions in neural networks, especially in binary classification problems. It takes any real-valued input and squashes it into a range between 0 and 1, which makes it useful for representing probabilities.When the input is a large positive number, the output gets close to 1, and when it is a large negative number, the output approaches 0. However, one limitation of the sigmoid function is that it can cause the <font color = red>vanishing gradient problem </font> during training, which slows down learning for deep networks.\n\n **Vanishing gradient problem** - happens when the gradients (which tell the network how much to adjust the weights during training) become extremely small as they move backward through the layers of a neural network.\n\n In the case of the sigmoid function, when the input value is very large or very small, the slope (derivative) of the sigmoid becomes almost zero because the output is close to 1 or 0 and the curve flattens out. During backpropagation, these tiny gradients are multiplied layer by layer, and as a result, they get smaller and smaller. This means the earlier layers of the network receive almost no updates — they “learn” extremely slowly or sometimes not at all.\n\n In simpler terms, the vanishing gradient problem makes it difficult for deep neural networks using sigmoid activation to learn effectively, especially in the deeper layers. That’s one reason why activation functions like ReLU (Rectified Linear Unit) are preferred in modern deep learning — they help keep gradients from vanishing.","metadata":{"id":"HVcBTWKc-9Th"}},{"cell_type":"code","source":"def relu(z):\n    return np.maximum(0, z)","metadata":{"id":"vre8-8jg7lg0","trusted":true,"execution":{"iopub.status.busy":"2025-10-15T19:13:35.757904Z","iopub.execute_input":"2025-10-15T19:13:35.758250Z","iopub.status.idle":"2025-10-15T19:13:35.762960Z","shell.execute_reply.started":"2025-10-15T19:13:35.758228Z","shell.execute_reply":"2025-10-15T19:13:35.761835Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def relu_deriv(z):\n    return (z > 0).astype(float)","metadata":{"id":"mVpIqYpc7lfL","trusted":true,"execution":{"iopub.status.busy":"2025-10-15T19:13:37.565146Z","iopub.execute_input":"2025-10-15T19:13:37.565422Z","iopub.status.idle":"2025-10-15T19:13:37.570888Z","shell.execute_reply.started":"2025-10-15T19:13:37.565405Z","shell.execute_reply":"2025-10-15T19:13:37.569918Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The binary cross-entropy loss (also called log loss) is one of the most common loss functions used for binary classification problems — where the output can be either 0 or 1. Its formula is written as:\n\n$$\nL = -\\frac{1}{m} \\sum_{i=1}^{m} \\Big[ y_i \\log(\\hat{y}_i + \\varepsilon) + (1 - y_i)\\log(1 - \\hat{y}_i + \\varepsilon) \\Big]\n$$\n\nwhere\n\n- m - total number of samples\n- $y_i$ - true label of the ith sample (0 or 1)\n- $\\hat{y_i}$ - predicted probability for that sample\n- $\\epsilon$ - a very small constant\n\nIf the model predicts correctly with high confidence (e.g., $\\hat{y_i}=0.99$ when $\\hat{y_i}=1$ ) the logarithm term becomes small, meaning a low loss. f it predicts incorrectly, the log term explodes to a large value, meaning a high loss. This loss function encourages the model to assign high probabilities to correct classes while penalizing overconfident wrong predictions.","metadata":{"id":"flaJipz1Ak4I"}},{"cell_type":"code","source":"# Loss: binary cross-entropy\ndef compute_loss(y_true, y_pred):\n    m = y_true.shape[0]\n    eps = 1e-8\n    return - (1.0 / m) * np.sum(y_true * np.log(y_pred + eps) + (1 - y_true) * np.log(1 - y_pred + eps))\n","metadata":{"id":"9Q-iyztW7ldP","trusted":true,"execution":{"iopub.status.busy":"2025-10-15T19:13:40.794924Z","iopub.execute_input":"2025-10-15T19:13:40.795278Z","iopub.status.idle":"2025-10-15T19:13:40.801377Z","shell.execute_reply.started":"2025-10-15T19:13:40.795256Z","shell.execute_reply":"2025-10-15T19:13:40.800238Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"4GK0CbvIDO-g","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The next code chunk is **forward pass** function.\n\nThe forward pass is the process where input data flows through the neural network to generate predictions. It starts by computing the first linear transformation `z1 = X.dot(W1) + b1`, where the input features `X` are multiplied by the first layer’s weights `W1` and added to the bias `b1`. This produces the raw input to the first hidden layer, which then passes through the ReLU activation function `a1 = relu(z1)` to introduce nonlinearity.\n\nThe same steps repeat for the second hidden layer: `z2 = a1.dot(W2) + b2` computes the linear combination, and `a2 = relu(z2)` applies the activation.\n\nFinally, the output layer computes `z3 = a2.dot(W3) + b3`, followed by a sigmoid activation `a3 = sigmoid(z3) to produce values between 0 and 1, representing the predicted probabilities for the target class.","metadata":{"id":"ngZ3gSE8DOew"}},{"cell_type":"code","source":"# Forward pass (returns intermediates needed for backprop)\ndef forward(X):\n    z1 = X.dot(W1) + b1          # (m, n_h1)\n    a1 = relu(z1)\n    z2 = a1.dot(W2) + b2         # (m, n_h2)\n    a2 = relu(z2)\n    z3 = a2.dot(W3) + b3         # (m, 1)\n    a3 = sigmoid(z3)\n    cache = (z1, a1, z2, a2, z3, a3)\n    return a3, cache\n\n# The function also stores all intermediate results—z1, a1, z2, a2, z3, and\n# a3—in a variable called cache, which is later used during backpropagation to\n# compute gradients.","metadata":{"id":"Mk6kZfZv7lbp","trusted":true,"execution":{"iopub.status.busy":"2025-10-15T19:13:44.089057Z","iopub.execute_input":"2025-10-15T19:13:44.089387Z","iopub.status.idle":"2025-10-15T19:13:44.097265Z","shell.execute_reply.started":"2025-10-15T19:13:44.089365Z","shell.execute_reply":"2025-10-15T19:13:44.095999Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"XcWlrCuKAkUN","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The next process is backpropagation algorithm.\n\nBackpropagation is the method by which the network computes gradients of the loss with respect to each weight and bias, allowing the parameters to be adjusted in the direction that reduces the loss.\n\nThe function starts by unpacking the cached intermediate results from the forward pass `(z1, a1, z2, a2, z3, a3)` and calculating the number of samples `m`.\n\n1. **Output layer gradients**:\n - `dz3 = a3 - y` just measures the difference between the predicted probability and the true label — basically “how wrong the output is.”\n - `dW3` and `db3` tell us how much the weights and biases in the output layer contributed to that error. We use them to update the weights so the output gets closer to the true label.\n\n 2. **Second hidden layer**:\n - The error from the output layer is sent backward: `da2 = dz3.dot(W3.T)` shows how much each neuron in the second hidden layer caused the output error.\n - Multiplying by `relu_deriv(z2)` adjusts for the ReLU activation, because ReLU only passes gradients for positive values.\n - `dW2` and `db2` then tell us how much to change the weights and biases in this layer.\n\n 3. **First hidden layer**:\n - Same idea: `da1 = dz2.dot(W2.T)` passes the error back one more layer.\n - Multiply by `relu_deriv(z1)` to adjust for ReLU.\n - `dW1` and `db1` give the updates for the first hidden layer weights and biases.\n\n\n---\n**Key Points:**\n- Start at the output: see the error.\n\n- Pass the error backward, layer by layer, adjusting for the activation function.\n\n- Compute how much each weight/bias contributed to the error.\n\n- Update the weights/biases to reduce the error.\n\nIn one sentence: backpropagation is just figuring out “who caused the error” in the network and nudging those weights in the right direction.\n\nAfter buildin the network structure, the next part is making predictions on the train data and then train the data based on the results.\n","metadata":{"id":"07HRAJSFEK3O"}},{"cell_type":"code","source":"","metadata":{"id":"zwKku28PFn2C","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Backprop and parameters update (full-batch gradient descent)\ndef backward_and_update(X, y, cache, lr=0.01):\n    global W1, b1, W2, b2, W3, b3\n    m = X.shape[0]\n    z1, a1, z2, a2, z3, a3 = cache\n\n    # Output layer gradients\n    dz3 = a3 - y                       # (m,1)\n    dW3 = (a2.T.dot(dz3)) / m          # (n_h2, 1)\n    db3 = np.sum(dz3, axis=0, keepdims=True) / m\n\n    # Layer 2\n    da2 = dz3.dot(W3.T)                # (m, n_h2)\n    dz2 = da2 * relu_deriv(z2)         # (m, n_h2)\n    dW2 = (a1.T.dot(dz2)) / m          # (n_h1, n_h2)\n    db2 = np.sum(dz2, axis=0, keepdims=True) / m\n\n    # Layer 1\n    da1 = dz2.dot(W2.T)                # (m, n_h1)\n    dz1 = da1 * relu_deriv(z1)         # (m, n_h1)\n    dW1 = (X.T.dot(dz1)) / m           # (n_x, n_h1)\n    db1 = np.sum(dz1, axis=0, keepdims=True) / m\n\n    # Update params\n    W3 -= lr * dW3\n    b3 -= lr * db3\n    W2 -= lr * dW2\n    b2 -= lr * db2\n    W1 -= lr * dW1\n    b1 -= lr * db1\n","metadata":{"id":"fAUsdB-bAkPk","trusted":true,"execution":{"iopub.status.busy":"2025-10-15T19:13:49.431666Z","iopub.execute_input":"2025-10-15T19:13:49.431985Z","iopub.status.idle":"2025-10-15T19:13:49.440366Z","shell.execute_reply.started":"2025-10-15T19:13:49.431957Z","shell.execute_reply":"2025-10-15T19:13:49.439060Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"MI5V0g6DAkLl"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"AiLMv8EDHU_N"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"For the prediction part we will have two functions:\n\n- `predict_proba(X)` : runs a forward pass through the network using the input X and returns the output layer activations a3. Since the output layer uses a sigmoid, these values are probabilities between 0 and 1, representing the model’s confidence that each sample belongs to class 1.\n\n- `predict(X, threshold=0.5)`: uses `predict_proba(X)` to get the probabilities, then converts them into class labels. It compares each probability to the threshold (default 0.5): if the probability is greater than or equal to 0.5, the function assigns it to class 1; otherwise, it assigns class 0. The .astype(int) ensures the output is an integer array of 0s and 1s.\n\nIn short, predict_proba will give the probabilities, while predict will return binary class predictions, which are useful for computing accuracy or making final decisions.","metadata":{"id":"aZjEYyOwHWJu"}},{"cell_type":"code","source":"# Helpers: predict probabilities and classes\ndef predict_proba(X):\n    a3, _ = forward(X)\n    return a3\n\ndef predict(X, threshold=0.5):\n    probs = predict_proba(X)\n    return (probs >= threshold).astype(int)\n","metadata":{"id":"lCDaDYl2HQca","trusted":true,"execution":{"iopub.status.busy":"2025-10-15T19:13:52.964525Z","iopub.execute_input":"2025-10-15T19:13:52.964978Z","iopub.status.idle":"2025-10-15T19:13:52.970161Z","shell.execute_reply.started":"2025-10-15T19:13:52.964953Z","shell.execute_reply":"2025-10-15T19:13:52.969070Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now it is time to write code for training.\n","metadata":{"id":"fBrjffwLI-tb"}},{"cell_type":"code","source":"# Training loop\n# -------------------------\nepochs = 1500\nlr = 0.05   # you can reduce if loss diverges\nprint_every = 100\n\nfor epoch in range(1, epochs + 1):\n    # forward\n    y_hat_train, cache = forward(X_train)\n    loss = compute_loss(y_train, y_hat_train)\n\n    # backward + update\n    backward_and_update(X_train, y_train, cache, lr=lr)\n\n    # logging\n    if epoch % print_every == 0 or epoch == 1:\n        train_preds = predict(X_train)\n        test_preds  = predict(X_test)\n\n        train_acc = np.mean(train_preds == y_train)\n        test_acc  = np.mean(test_preds == y_test)\n\n        print(f\"Epoch {epoch:4d} | loss: {loss:.4f} | train_acc: {train_acc:.4f} | test_acc: {test_acc:.4f}\")\n","metadata":{"id":"-SFpIxkDHQTq","outputId":"e4c9a5b6-7afe-4f3a-86b4-f93602651b30","trusted":true,"execution":{"iopub.status.busy":"2025-10-15T19:13:59.204269Z","iopub.execute_input":"2025-10-15T19:13:59.204580Z","iopub.status.idle":"2025-10-15T19:13:59.884863Z","shell.execute_reply.started":"2025-10-15T19:13:59.204558Z","shell.execute_reply":"2025-10-15T19:13:59.883605Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The above code consists of the following parts:\n1.  <font color = darkcyan><b>Set hyperparameters:</b></font>\n- `epochs` sets how many times the network will see the entire training dataset. More epochs allow more learning but take longer.\n- `lr (learning rate)` controls the size of the steps taken during weight updates. Too large can make the loss explode; too small makes learning slow.\n- `print_every` determines how often we print progress to monitor training.\n\n2. <font color = darkcyan><b>Loop over epochs:</b></font>\n\n- We iterate from 1 to epochs so that each pass through the loop is one full training cycle over the dataset.\n\n3.  <font color = darkcyan><b>Forward pass:</b></font>\n- `forward(X_train)` computes the network’s predictions for all training samples.\n- `y_hat_train` contains the predicted probabilities for each sample.\n- `cache` stores intermediate values needed for backpropagation.\n- `compute_loss` calculates the binary cross-entropy loss, measuring how far predictions are from the true labels.\n\n4.  <font color = darkcyan><b>Backward pass and update:</b></font>\n- This function computes the gradients of the loss with respect to all weights and biases (backpropagation) and updates them using gradient descent with learning rate lr. This is the step where the network “learns.”\n\n5.  <font color = darkcyan><b>Logging / monitoring:</b></font>\n\n- Every print_every epochs (and the first epoch), we check how well the network is performing:\n    - `predict(X_train)` converts probabilities into class labels for the training set.\n    - `predict(X_test)` does the same for the test set.\n\n6.  <font color = darkcyan><b>Accuracy calculation:</b></font>\n- We compute the fraction of correctly predicted samples for training and test sets. This is a simple measure of model performance.\n\n7.  <font color = darkcyan><b>Printing progress:</b></font>\n\n- This line prints a summary showing:\n    - The current epoch\n\n    - Training loss\n\n    - Training accuracy\n\n    - Test accuracy\n\n    It helps track if the network is learning over time, if loss is decreasing, and whether the model is overfitting (training accuracy high but test accuracy low).","metadata":{"id":"_6gaX95aJXCO"}},{"cell_type":"code","source":"# Final evaluation\ntrain_probs = predict_proba(X_train)\ntest_probs  = predict_proba(X_test)\n\ntrain_acc = np.mean((train_probs >= 0.5).astype(int) == y_train)\ntest_acc  = np.mean((test_probs  >= 0.5).astype(int) == y_test)\nprint(\"\\nFinal -> Train accuracy: {:.4f} | Test accuracy: {:.4f}\".format(train_acc, test_acc))\n","metadata":{"id":"O8JT4_mhJQ_0","outputId":"2e3c8c62-e118-495c-fce8-086496daef82","trusted":true,"execution":{"iopub.status.busy":"2025-10-15T19:14:06.713967Z","iopub.execute_input":"2025-10-15T19:14:06.714379Z","iopub.status.idle":"2025-10-15T19:14:06.721699Z","shell.execute_reply.started":"2025-10-15T19:14:06.714353Z","shell.execute_reply":"2025-10-15T19:14:06.720845Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Accuracy alone is often not enough for evaluating classification models, especially when the classes are imbalanced, like in many medical datasets (including Cleveland heart disease). Accuracy just measures the fraction of correct predictions, but it doesn’t tell you how well the model handles false positives and false negatives, which is where metrics like precision, recall, F1-score, ROC AUC, and Gini coefficient come in.","metadata":{"id":"L6bXUbovPKZe"}},{"cell_type":"code","source":"from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n\n# Convert probabilities to class predictions\ntrain_preds = (train_probs >= 0.5).astype(int)\ntest_preds  = (test_probs >= 0.5).astype(int)\n\n# Precision, Recall, F1\ntrain_precision = precision_score(y_train, train_preds)\ntrain_recall    = recall_score(y_train, train_preds)\ntrain_f1        = f1_score(y_train, train_preds)\n\ntest_precision  = precision_score(y_test, test_preds)\ntest_recall     = recall_score(y_test, test_preds)\ntest_f1         = f1_score(y_test, test_preds)\n\n# ROC AUC and Gini\ntrain_auc = roc_auc_score(y_train, train_probs)\ntest_auc  = roc_auc_score(y_test, test_probs)\n\ntrain_gini = 2 * train_auc - 1\ntest_gini  = 2 * test_auc - 1\n\n\n\n\n# Create a dictionary with metrics\nmetrics_dict = {\n    \"Train\": [train_acc,train_precision, train_recall, train_f1, train_auc, train_gini],\n    \"Test\":  [test_acc,test_precision, test_recall, test_f1, test_auc, test_gini]\n}\n\n# Index for the DataFrame\nmetrics_index = [\"Accuracy\", \"Precision\", \"Recall\", \"F1-score\", \"ROC AUC\", \"Gini\"]\n\n# Create DataFrame\nmetrics_df = pd.DataFrame(metrics_dict, index=metrics_index)\n","metadata":{"id":"9rcBOCgrPJfv","trusted":true,"execution":{"iopub.status.busy":"2025-10-15T19:14:17.616249Z","iopub.execute_input":"2025-10-15T19:14:17.616588Z","iopub.status.idle":"2025-10-15T19:14:17.651319Z","shell.execute_reply.started":"2025-10-15T19:14:17.616564Z","shell.execute_reply":"2025-10-15T19:14:17.649163Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"metrics_df","metadata":{"id":"n8sOK6V9PJdW","outputId":"968d7cdc-875b-4285-c59b-14766745adbe","trusted":true,"execution":{"iopub.status.busy":"2025-10-15T19:14:20.403614Z","iopub.execute_input":"2025-10-15T19:14:20.403968Z","iopub.status.idle":"2025-10-15T19:14:20.413974Z","shell.execute_reply.started":"2025-10-15T19:14:20.403942Z","shell.execute_reply":"2025-10-15T19:14:20.412883Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Looking at these results, we can summarize the neural network’s performance on the Cleveland heart disease dataset as follows:\n\n<font color = skyblue><b>Accuracy:</b></font> The neural network correctly predicts the class for about 89% of the training samples and 83% of the test samples. This indicates good overall performance, though the slightly higher training accuracy suggests minor overfitting. Accuracy alone is useful, but in medical datasets, other metrics like recall and precision are more critical.\n\n<font color = skyblue><b>Precision:</b></font> The model is slightly better at avoiding false positives on the training set (0.87) than on the test set (0.80). This means when it predicts a patient has heart disease, it is more often correct on the training data.\n\n<font color = skyblue><b>Recall:</b></font>  The model detects most of the actual positive cases, with very high recall on both training (0.93) and test (0.88). This is good for medical applications where missing positive cases is risky.\n\n<font color = skyblue><b>F1-score:</b></font>  Balancing precision and recall, the F1-score is strong on both training (0.90) and test (0.84), showing overall good predictive power.\n\n<font color = skyblue><b>ROC AUC:</b></font>  High AUC values (0.95 train, 0.90 test) indicate the model can distinguish between patients with and without heart disease effectively.\n\n<font color = skyblue><b>Gini coefficient:</b></font>  Also high (0.90 train, 0.80 test), reinforcing strong discriminative ability.\n\n<font color = steelred><b>Summary insight:</b></font>  The model performs very well overall, with slightly better results on the training set, which is expected. There is a small gap between train and test metrics, suggesting minor overfitting, but the test performance remains strong, especially recall and AUC, which are critical in a medical context to catch as many true cases as possible.","metadata":{"id":"GT74DdP_P224"}},{"cell_type":"code","source":"","metadata":{"id":"EPAe3vi5PJbE"},"outputs":[],"execution_count":null}]}